\documentclass{amsart}
\usepackage{graphicx}
\graphicspath{{./}}
\usepackage{hyperref}
\usepackage{csvsimple}
\usepackage{longtable}
\usepackage{lscape}
\usepackage{epigraph}
\title{Some of My Older Thoughts about Paul Levy} 
\author{Zulfikar Moinuddin Ahmed}
\date{\today}
\begin{document}
\maketitle

\section{Wed, Nov 28, 2018, 10:39 AM}
ON MEASURE THEORY IN SCIENCE

The value of measure theory per se is unclear to me.  I think that it's very easy to get lost in technicalities of measure theory that are completely useless.  On the other hand, the actual universe is a hypersurface evolving in a round four-sphere and some things which we want to take for granted should be considered carefully.  First of all, although I do not believe that loop quantum gravity theory will lead anywhere since gravity is actually just a secondary effect of a four-dimensional electromagnetism, there is the question of ROUGHNESS of the physical universe in the smooth four-sphere universe and measure theory is then necessary to deal with the description of the physical universe.  On the other hand abstract measure theory is just not interesting in itself.  I would need to think about this more; it's an interesting question.  For example, I believe that MARKOV processes are not quite the right models for most natural processes but the non-Markovianity of Nature seems to be just time-fractional.

I guess if I had to pick the most important class of stochastic processes for nature I would consider some sort of time-fractional Levy processes with jumps as the class of stochastic process that are most likely to cover most of natural phenomena close to exactly.  Analysis of these process requires sophisticated measure theory.  

But remember that I like to swap the importance of mathematics and science in the sense that I would suspect that ALTHOUGH we have traditionally looked upon BROWNIAN MOTION and POISSON PROCESS as mathematically central models, and diffusion processes too I think that in the end we will find that time-fractional Levy processes should have been our primary concern in mathematics to begin with.  This is not such a strange thing at all because if PAUL LEVY had accepted the primacy of Brownian motion and Poisson Process probability theory would not have had deep development in the twentieth century.  I have here Sato's book, Applebaum's book and a great special collection of work on Levy Processes and as I complain and whine about what needs to happen in Global Central Banking -- how we need a Global Central Bank and how we need to manage the global economic monetary policy using extra control of global unemployment, global inflation and other Happiness metrics -- I am mindful of purely mathematical concerns too.  I do not like to work on purely mathematical problems until I am fully convinced that there is something deep ABOUT ACTUAL NATURE that is being explored by the mathematics.  This is not quite an APPLIED perspective; rather it is a HARDCORE CONVICTION that mathematics that is not digging into actual features of nature is not mathematics that is all that important and will be redone and vanish from existence and collect dust.  Let me just point out that there are 2400+ papers solving Einstein's gravitational field equations.  And now I know the GR equation is not describing any dynamics of nature at all; rather it is an approximation and the real dynamics of all things in the universe is just electromagnetism.  So what will happen to these 2400 papers?  People might read some of them if they are particularly curious but by and large they will just be forgotten as efforts of man's genius to solve certain types of problems.  Life is too short to work on problems that are guaranteed to become irrelevant.

\section{Wed, Nov 28, 2018, 11:33 PM}
HERE MUST RESEARCH IN PROBABILITY THEORY GO

Ladies and Gentlemen,

I have a great deal of respect for my ex-advisor DAN STROOCK as a mathematical genius and not just because of the particular things he has done in his research.  We and S. R. S. Varadhan had pioneered the very powerful 'martingale problem' approach to study of stochastic processes.  And I have not been involved in mathematical research for a couple of decades but would like to actually return to mathematical research.  But my own genius is not in mathematical technical issues although I can brush up as necessary.  I have a new philosophy about mathematics and science that is unorthodox as is natural for me since I am showing the world the correct geometric unity in Nature in the four-sphere cosmos.  So I want to take a holistic approach again to stochastic processes.  I look at stochastic process theory from a distance again and I see the beginnings in 1900 with Louis Bachelier's invention of the Brownian motion on one hand and on the other hand I look at my work in finance and what I find is the absolute importance of LONG MEMORY.  I have right next to me the monumental tome of KEN-ITO SATO "Levy Processes and Infinitely Divisible Distributions" which contains within it more or less everything that mathematicians know about Levy processes.  These processes were of extreme importance for a very long time.  Professor Stroock's first lucid lecture by HENRY MCKEAN was on infinitely divisible laws and their deep study was of course done by the great mathematician PAUL LEVY.  But I want to approach the entire field with a negative issue -- that MARKOV PROCESSES are not quite the right processes that should belong to the center of stochastic process theory.  The center of attention in stochastic process theory itself can be seen as the end point of the Levy process research.  The central objects should have been TIME-FRACTIONAL LEVY PROCESSES all along throughout the twentieth century.  Of course one can consider these starting from Markov processes and say that the unity of the subject is in the link of stochastic processes driven by the Brownian motion to PARABOLIC partial differential equations.  But parabolic equations are frankly the toys from MACROSCOPIC NATURAL PROCESSES.  The general equation that actually matters for natural systems has the form

\[
(d/dt)^{\alpha} - \Delta^{\beta} = 0
\]
The study of these fractional partial differential equations has been a subject of recent mathematical interest.  Now looking at my own experience, this is empirically the most useful differential equation in science.  I do not believe there is a single macroscopic sytem where fine tuned fit to data will produce alpha=1 which corresponds to a parabolic partial differential equation.  One can see diffusion theory as warm up exercise to precise confrontation with these equations.  So this is the sharp bridge between mathematics and science in the empirical science side.  Of course PDE theorists have been busy with these and other equations as have been the probability theorists so I am not saying something that is quite UNIQUE but I do want to make this very clear to those who are not aware of this:  NO ACTUAL MACROSCOPIC PHYSICAL SYSTEM WITH TIME DEPENDENCE ever follows the parabolic equation exactly with alpha=1 whenever any amount of complexity pops in.  The ideal heat flow models are just toy models.

In particular, I want to give proper credit from my own perch (however far that perch is from the Nobel Prize winners of the wrong science that goes today by the name of physics) that the determination of the FUNDAMENTAL SOLUTION of these time-fractional equations is a great achievement comparable to the first discovery of fundamental solution of the heat equation by MAINARDI, LUCHKO, PAGNINI.  I am interested in making the connection with the correspondence between the established theory of stochastic Markov processes with parabolic partial differential equation over to those of these fractional equations and time-fractional Levy processes as I learn more about research already done.  I will admit that RIGOR is not top on my mind as I have to rush to the analogue of Hamilton-Jacobi-Bellman equation in this case because on my mind is the application to CENTRAL BANKING control using these tools.  I am hoping that the research has already been advanced by probability theorists and analysts.  But if there are problems this is my re-entry point for mathematics.  Unlike established mathematicians, I take an extremely coercive unified view of science and mathematics.  I believe in the total unity of Nature and I have very good backing for this.  HERMANN WEYL and ALBERT EINSTEIN both took unity of science and mathematics seriously.  I do as well and in this case, I think that time-fractional Levy processes is SCIENCE telling MATHEMATICS what should be mathematically interesting.  We should re-examine ideally all of probability theory and clean up our mathematical understanding of these guys, these time-fractional Levy processes to find new heights of understanding of the objects and properties of the purpose of stochastic processes.

\section{Thu, Nov 29, 2018, 8:32 AM}
WHY SHOULD TIME-FRACTIONAL LEVY PROCESSES BE THE CENTRAL OBJECTS OF PROBABILITY THEORY?

Ladies and Gentlemen,

Before we get deep into precise theorems which we invariably have to, we want to understand why MATHEMATICIANS should consider TIME-FRACTIONAL LEVY PROCESSES to be fundamental to Mathematical Probability Theory.  Here is a very intuitive answer to this.  Dan Stroock's beautiful graduate treatment of analytic probability theory describes the infinitely divisible distribution in terms of distributions whose Fourier transforms are powers of a function which is itself a Fourier transform of a probability distribution.  These are the distributions of various sorts of sums of independent identical shocks added up.  Dan Stroock also describes PAUL LEVY's dynamical view of these infinitely divisible distributions which are the Levy processes.  Levy processes have had deep and beautiful properties discovered -- one of the most fruitful areas of mathematical probability theory.  But here is WHY the time-fractional part belongs squarely even more centrally to mathematics than the LEVY PROCESSES themselves:

The issue is to consider the sums of independent set of independent identically distributed random variables WITH DIFFERING SPEEDS summed up.  This puts the time-fractional Levy process squarely in the center of fundamental mathematical study again because the laws of such atoms are precisely the objects of probability theory.  The stochasticity of speeds in the atoms of a stochastic process is the source of the time-fractional nature of all macroscopic timed systems and they are exactly what needs to be carefully centered for mathematical reflection in probability theory.  I mean that there is 'long memory' is not quite specific enough for the universal feature that occurs in timed measurements of complex systems.  What occurs is time-fractionation.  The atoms have differing speeds to build these time-fractional Levy processes.  These need a theory from scratch really so there should be some fundamental Levy Khinchine formua that brings them out directly so we can understand what these mathematical discoveries were really telling us all along.  In other words the entire theory of Levy processes should be really seen as the preliminary pass for all techniques needed to study time-fractional Levy processes which are really much more central to nature and probably the REAL objects mathematicians should have known about in hindsight.  I am convinced that Nature knows more about what mathematics ought to be than humans do.

\section{Fri, Nov 30, 2018, 6:04 PM}
THE DIFFERENCE BETWEEN JUMP-DIFFUSIONS AND LEVY PROCESSES

Ladies and Gentlemen,

A diffusion is a process with a generator that is an elliptic operator.  An elliptic operator is just a differential operator whose symbol is positive definite.  Recall how a symbol of a differential operator is computed -- the recipe is to take polynomials in $d/dx_i$ and replace these by variables $y_i$ on $\mathbf{R}^n$ and the symbol is this function.  The connection between stochastic processes and partial differential equation comes, as is well-known, from the fact that the HEAT EQUATION Cauchy problem 
\[
d/dt = \Delta
\]
with initial conditions can be solved microscopically by the path of a single particles by Brownian motion.  This generalizes to stochastic differential equations on $\mathbf{R}^n$

\[
dX_t = \mu(t,X_t) dt + \sigma(X_t,t)*dW_t
\]
where $W_t$ is the Brownian motion on $\mathbf{R}^n$.  The key tool in stochastic analysis of diffusions is the ITO formula which is just the analogue of fundamental theorem of calculus for diffusions, 

\[
df(W_t) = f_t(W_t) dt + 1/2 (\Delta f)(W_t).
\]
  
The entire subject of stochastic calculus for diffusions centers around clever use of this formula.  

Empirically, diffusions are not enough -- I know this story for price processes in markets introducing JUMPS in the diffusions went back to ROBERT MERTON in the 1970s and there are all sorts of studies that tell you that you needed jumps in price returns processes, you need stochastic volatility so volatility itself is a stochastic process, and then you need jumps in return process and you also need jumps in the volatility process.  That was my starting point for my SV models which added in long memory and got the best fits in the world.

But of course PAUL LEVY had considered 'jump diffusions' already in the 1930s.  So Levy processes are jump diffusions with a fixed distribution for the jumps and this distribution is called the Levy measure.  

The Levy processes are jump-diffusions with a precise model included for the jumps with total independent increments and stationary distribution.  

When I say that time-fractional Levy processes are the ZEROTH order models for all timed measurements of complex systems in nature, to put in context what higher order models might be these would be dynamical evolution losing stationarity, so Feller processes.

Consider macroeconomic data -- there is not enough data to justify too much dynamism often.  This is of course going to be the complaint of the EDWARD PRESCOTT type on central bank decisions solely based on models of this type but the 0<alpha<1 from time-fractional part is strictly necessary in all fields.  

\section{Dec 3, 2018, 1:31 AM}
A BIT DEEPER PUSH INTO THIS THING OF WHAT'S WRONG WITH STATS WAY OF DEALING WITH TIME SERIES

Ladies and Gentlemen,

So the the great man of statistics is without any controversy RONALD FISHER.  The man invented likelihood and maximum likelihood estimation and I was trying to read his book years ago when I was working for a biotech in San Francisco.  Somehow there is something not quite right about this when you think about time series.  I am of course biased about this because now I am totally obsessed by trying to understand equations like
\[
(d/dt)^{\alpha} = \Delta
\]

\section{Mon, Dec 3, 2018, 3:06 AM}
NATURE CARES ABOUT NON-STABLE LEVY PROCESSES

Ladies and Gentlemen,

The full gamut of processes for noises in nature is known by examples from all different fields to be unconstrained by 'stability' or 'self-similarity' as the attached paper discusses.  I am scanning through these papers to understand the requirements of a totally universal multivariate time series model by the simple criterion of crossing out unnecessarily restrictive features.  For me it is quite incredible that Paul Levy in 1930 hit upon the characterization of infinitely divisible distributions without the guidance that seems to be provided directly from nature by pure mathematical genius and for most of the past half a century this wild explosion of noises was not quite grasped correctly because I think lack of understanding and examples but also because it's hard to stray very far away from Gaussian noise psychologically and there is also the mathematical demands.  

I think the right thing to do is conjecture that basically the entire class of Levy noises can be achieved in various actual time series measurements.  Benoit Mandelbrot attempted a restriction to alpha-stable processes and the paper below tells us that this was premature.  

Now a generic model for Levy measure is going to be impossible to use for general purpose statistical model for time series that is universally valid.  My particular interest is now in the global macroeconomic variables but I am attempting to formulate at least qualitatively a general idea for what jump measures would be sufficiently wide as to accomodate vector time series across the board in all fields.  I am running ARFIMA fitting for the past few hours on 151x12 dataset and this is already quite demanding computationally.  So the fast computation is a necessity as well but I believe that the human race needs time series models that are going to cover the noises that actually occur in nature.  This is something that is becoming clear from may areas of science where there needs to be a solid continuous time time-fractional Levy driven model.  This was definitely the case in financial prices but it would be a huge boon to humanity to get a universal model that is zeroth order correct with ability to handle jumps without a combinatorial mess and impossibly long run times for estimation.  I would personally have loved to be able to pull up multivariate time series data in Excel and get sharp fits with ability to forecast and do control problems without worry about the quality of the fit being far from the state of the art of human understanding.  I believe that baseline is a time-fractional multidimensional continuous time model with levy noises. And I think such a piece of software would be within reach and would displace many established time series models in use.  

In the sciences this is not as critical perhaps but in macroeconomics it is totally fatal because the noise drowns out signal so often.  But in a strange irony the mechanical engineers are much more adept at handling this than the economists by and large because there the problems are harder actually and there is no experimental clarity.   And ironically as well, here is where the errors have the most damning effect on the fortunes of th vulnerable across the globe.  So I am not doing this project only for technical kicks -- I believe that not only must macroeconomic models be re-evaluated with a stronger set of models but also that central banking must be unified and inflation targeting should be replaced by targeting a larger set of metrics.  There are 2.9 billion people living on less than \$2 a day and if a technical cleanup of all the bad macroeconomic models and a new direction in global monetary policy is all it takes to change this ghastly situation, then it's completely worthwhile pushing this with all one's efforts.

I consider it one of my greatest insights to see that global poverty and unemployment does have a solution and that is a macroeconomics that is tighter and central bank regulation of happiness metrics as well as global unemployment and poverty not just out of charity but also because I believe these variables will produce a healthier global macroeconomy.  I am not a socialist at all but I used to be when younger.  There needs to be some understanding that these free market religious zealots may have coherent theories but these theories are worthless when confronted with the varieties that make up this species, this race, of seven billion.

What does this have to do with Non-stable Levy processes?  Isn't it beautiful?
For the 'epic story of maximum likelihood' read the attached article which is a well written and entertaining piece.  So what occurs to me is that this maximum likelihood principle is not so much a marvel of SCIENCE as that of ENGINEERING with various justifications after the fact.  That it was a great moment in human civilization when Fisher produced his version is not in doubt.  And I don't have any intuition for these issues EXCEPT when it comes to time series data.  I tend to trust them in other contexts of other types of data without a second thought.

In time series however, there is the frequency spectrum and frequency analysis is quite important.  Just to make things concrete let's take a look at a spectrum for US inflation and unemployment.  There are some theorems that say maximum likelihood is the same whether done in time-space or frequency space.  But there are some doubts by some whether maximum likelihood estimation should be used for fitting stochastic volatility models with jumps for jump parameters.  

Now the way the practical way that I approach fitting time series models is that I look at the residuals and graph the ACF function and don't like the model very much when the ACF function has peaks at odd places.  

But for some reason I am totally convinced that this is an issue which is extremely central to mathematics, well specifically to stochastic process theory and that the spectral characteristics of theoretical Levy processes can be understood only by comparison to empirical frequency space distributions.  The goal is to seek the intersection of what Paul Levy and other probability theorists had discovered had theorems -- these Levy processes are extremely rich for theorems although I am getting back to thinking about these and am not so much interested in proving new theorems.  But just intuitively time-fractional Levy processes are naturally going to contain more theorems since these can be described as the random time change of a levy process by inverse stable subordinator but much much much more importantly, these things are universal models of actual time series observed and I claim are universal objects from nature's view.  Thus in a sense the development of stochastic process theory was ARTIFICIAL from Bachelier 1900 till we hit these time-fractional Levy processes and then this is the first time this stochastic process theory actually HITS NATURE SQUARELY.  Surely this then has to have enormous consequence for mathematics because we know from physics that when you veer off nature you lose you balance and get lost in space.  Hard ground for probability theory in my view did not EXIST before time-fractional Levy processes could be seen and this almost cries out for a deep re-examination of what stochastic process theory and probability theory is all about.

I am not able to do this yet but I know this is very deep.  I am not simply proposing to do the project of fitting these things to all time series as a random whim.  I am seeking solid ground.  Solid ground could lead to a renaissance in pure mathematics too.

\end{document}