\documentclass{amsart}
\usepackage{graphicx}
\graphicspath{{./}}
\usepackage{hyperref}
\usepackage{csvsimple}
\usepackage{longtable}
\usepackage{lscape}
\usepackage{epigraph}
\title{May 23 2021:  My Health Condition Degenerates Further} 
\author{Zulfikar Moinuddin Ahmed}
\date{\today}
\begin{document}
\maketitle

\section{Wake Up With Difficulties Breaking and Messed up Meta and Bill Gates Clearly intent on Continuing to Harm Me}

Since I arrived to America in 1987 to, say, 2019, I have never imagined a situation where someone in America of wealth and privilege literally is malevolent and spends his time violating people's Natural Rights in America and keeps getting accolades from Washington instead of just being taken to the wall and shot in the head.  We, America, have killed 60\% over homicides in the world since 1945, with invasions in 37 countries, and we can't kill a single criminal murderer, Bill Gates, who runs amok causing harm and misery in America.  That is just outrageously absurd.  I feel that maybe I could just label the geographic area where Bill lives as a foreign country with oil underground and immediately we will invade him without any reason and occupy his land.

Bill Gates has literally committed a {\em crime against humanity} by Geneva Conventions and Hague consider by harming my interior and body meta and yet, this horrific, malevolent, criminal, abomination, this murderer still is free in America while he is continuing to harm me every single day and night and the US Senate is silent, unresponsive, and they think that this will not permanently destroy their family name, not put them as moral equivalent of Nazi war criminals in the eyes of the future.   Just the mere thought of what Nazis could have done to Albert Einstein produces horrors, while I, a great genius who have overthrown Einstein and Erwin Schroedinger, is mistreated in my own country where a murderer is considered to be more important to the country than myself.  Wonders never cease when America is involved.  To say that I am outraged at the mute response of US Government at continuing horrors in my life due to their wrong-headed actions is an understatement.

\section{Substantial Work}

I am considering the issue of statistical inference of eleven variables, Q7-Q17, from Wave 7 of World Values Survey.  The data are beautiful. Total sample is $N=19077$.  I am fascinated by the issues of statistical estimation of the true value of the Bernoulli parameters for these data.  

\section{Latent Exponential Model for Q7-Q17}

We consider a latent exponential model.  Even though the data is Bernoulli, we consider the following generation of the data.

People choose values in $[0,1]$ according to the truncated Exponential distribution.
\[
X \sim \frac{\lambda e^{-\lambda t}}{1-e^{-\lambda}}
\]
Then their choice is transformed by the function
\[
\zeta( x ) = 
    \begin{cases}
      0, & \text{if}\ x \le 1/2 \\
      1, & \text{otherwise}
    \end{cases}
\]
The actual labelling of data is 1/2.  We use the convention that $x=0$ represents the end of the higher mass of the two.

\section{All Things Fleeting Are Washed Away in Tides of Fate}

Some things persist in Human Affairs.  Other things are washed away like sandcastles on the beach.  When I see these things, I am filled with melancholy for I know deep in my heart that they will be swept away in time, as Nature's forces will wash them away.  This is why even in Mathematics, I was not enthusiastic about many topics.  I remember sitting in the Mathematics Library at Columbia, and reading many things in Mathematics.  In the winter, it was comfortable, and quiet, behind the stacks.  The journals were lined up in the shelves, and I marvelled at the production of Man.  I was interested in many topics then.  I was happy in marriage and lived in 113th street.  Years later I am still impressed by just how much of the Mathematical ouvre has been forgotten.  Millions of pages of work replaced by clearer understanding by later generations, much no longer considered anything but technicality, of historical interest alone.  I consider things that persist even for five centuries in Human Affairs, and they too are fleeting and will be washed away as well.  I am instinctively resistant to evade immortality and I am not interested in things that will lose their importance in five centuries.  And Man cannot give this to me.  Perhaps this is the reason for my fascination with Rilke.  "Ah, whom can we turn to in our need?  Not angels, not humans, and already the knowing animals are aware that we are not really at home in our interpreted world".  I am motivated my immortality, and there is nothing special, for we are all motivated by our immortality.  What is interesting, however, is that it is very clear to me now that this is a fundamental motivation that persists, is as human as one can imagine.  

\section{Correspondence between Bernoulli and Exponential}

\[
p = \frac{1-e^{-\lambda/2}}{1-e^{-\lambda}} = \frac{1}{1+e^{-\lambda/2}}
\]

And

\[
\lambda = - 2 \log\left( \frac{1-p}{p} \right) = 2 \log( p/(1-p))
\]
This is the log-odds.  Generalised Linear Models with binomial family is the appropriate statistical model.  This is a beautiful connection between fairly sophisticated statistical models and pure mathematics that ought to be clarified and explored by mathematicians as well.

\section{Creating Artificial Exponential Data}

\begin{verbatim}
# We will use logistic regression
# with x variable artificially created for Q7-Q17
# We take N=500 points to determine p, lambda
# Then we use these to assign random x values
# for all the other values
# then we fit logistic regression on the (x,g)

samp.binary.exp<-function( grps, lambda ){
  n <- length(grps)
  gvals <- unique(grps)
  bval <- 0
  sval <- 1
  if ( length(gvals) == 2 ){
    if ( sum(gvals==gvals[1]) >= n/2 ){
      bval <- gvals[1]
      sval <- gvals[2]
    } else {
      sval <- gvals[1]
      bval <- gvals[2]
    }
  } else {
    return(NULL)
  }
  xs<- rep( 0, n)
  for ( r in 1:n ){
    done <- F
    
    while ( done == F){
      pickx <- rexp(lambda)
      if (pickx <= 5){
        if ( grps[r] == sval){
          xs[r] <- pickx
          done <- T
        }  
      }
      if (pickx > 5){
        if ( grps[r] == bval){
          xs[r] <- pickx
          done <- T
        }  
      }
    }
  }
  xs
}
\end{verbatim}

We will create artificial data from the exponential distribution, and then assess the fit of a generalised linear model with the $x$ values and the Bernoulli variable as group labels.

\section{My Assessment of Bill Gates}

I am harmed but I also have a responsibility to my Beloved People, the Human Race.  The way I see this is that Bill Gates had been extremely malevolent against the interests of a man, myself, who had shared with him some of my best work in Finance, the Medium Frequency Strategy, that is the culmination of many decades in Finance, and instead of reciprocating, he cut into my eyes and destroyed my Ancestral, Family meta, and exercised enormous power against my interest and blocked my American Dream and violated my Natural Rights.  I am therefore obligated to ensure, both for my own sake and for the benefit of the Human Race, my recommendation that he be wiped out with lethal force, and all achievements of his burned to cinders and he be removed from History of Human Race altogether, and his reputation be scorched and his memory be erased from the Collective Human Consciousness.  One could consider this the bitter response from ressentiment, but I am superior to him morally and intellectually, and I have no envy for his money. Now \$120 million with D. E. Shaw \& Co. is not his money but my Private Property.  United States Government ought to wipe him out from existence.

\section{Structure of My Argument To Extinguish All Race Theories}

We will consider 11 variables Q7 to Q17 of World Values Survey.  For each, we will consider six ethnicity groups, (a) Arab, (b) Black, (c) East Asian, (d) Indian, (e) Other, (f) White.

There are seven series of Bernoulli values, totalling $N=19077$ points.  

For each we will find $\lambda$ such that logit model 
\[
p=(1+e^{\lambda/2})^{-1}
\]
Then we will consider statistics of these $\lambda$ for each of the variables and record the variation due to ethnicity.  

If we find that a small variation versus $\lambda_{HR}$ for $\lambda_1,\dots,\lambda_6$ results then we conclude that the influence of ethnicity on human race child rearing values is this standard deviation.  It this is smaller than say 30\% we can be assured that moral superiority or inferiority is inadmissible for the human race based on ethnicity.

\begin{verbatim}
# We will use logistic regression
# with x variable artificially created for Q7-Q17
# We take N=500 points to determine p, lambda
# Then we use these to assign random x values
# for all the other values
# then we fit logistic regression on the (x,g)

samp.binary.exp<-function( grps, lambda ){
  grps<-as.vector(t(grps))
  n <- length(grps)
  print(n)
  gvals <- unique(grps)
  #print(gvals)
  bval <- 0
  sval <- 1
  if ( length(gvals) == 2 ){
    if ( sum(gvals==gvals[1]) >= n/2 ){
      bval <- gvals[1]
      sval <- gvals[2]
    } else {
      sval <- gvals[1]
      bval <- gvals[2]
    }
  } else {
    return(NULL)
  }
  xs<- rep( 0, n)
  for ( r in 1:n ){
    done <- F
    
    while ( done == F){
      pickx <- rexp(1,rate=1/lambda)
      #print(pickx)
      if (pickx <= 5){
        if ( grps[r] == sval){
          xs[r] <- pickx
          done <- T
        }  
      }
      if (pickx > 5){
        if ( grps[r] == bval){
          xs[r] <- pickx
          done <- T
        }  
      }
    }
  }
  xs
}
\end{verbatim}

\section{Why is this Timeless?}

For about a millenia in the past, Human Race has been confused about the effects of ethnicity on moral qualities of individuals.  Our result will be valid for many millenia in the future.  Therefore it is not ephemeral, and this is the interest for me in these questions.

\section{Zulf Picks His Battles With Care}

There are numerous people with whom I could have chosen to be in conflict; for example, I could have been in conflict with Henry Jarecki for various reasons but I chose not to do this because it's not worth my time.  However, it is Bill Gates who forced my hand, and I decided that it was worth my time to pick a fight with Bill Gates and this is a wise choice.  I will prevail here of course because I am not rash and malevolent.  He is also plebian, so he will fall.

\section{Formality is Important}

We now have some code to test out our ideas about fitting Generalised Linear Models.  We're not seeing beautiful fits yet.

\begin{verbatim}
# We will use logistic regression
# with x variable artificially created for Q7-Q17
# We take N=500 points to determine p, lambda
# Then we use these to assign random x values
# for all the other values
# then we fit logistic regression on the (x,g)

samp.binary.exp<-function( grps, lambda ){
  grps<-as.vector(t(grps))
  n <- length(grps)
  print(n)
  gvals <- unique(grps)
  #print(gvals)
  bval <- 0
  sval <- 1
  if ( length(gvals) == 2 ){
    if ( sum(gvals==gvals[1]) >= n/2 ){
      bval <- gvals[1]
      sval <- gvals[2]
    } else {
      sval <- gvals[1]
      bval <- gvals[2]
    }
  } else {
    return(NULL)
  }
  xs<- rep( 0, n)
  for ( r in 1:n ){
    done <- F
    
    while ( done == F){
      pickx <- rexp(1,rate=1/lambda)
      #print(pickx)
      if (pickx <= 5){
        if ( grps[r] == sval){
          xs[r] <- pickx
          done <- T
        }  
      }
      if (pickx > 5){
        if ( grps[r] == bval){
          xs[r] <- pickx
          done <- T
        }  
      }
    }
  }
  xs
}

dataset.logit<-function(var,lambda){
  y<-na.omit(polv[,var])
  print(head(y))
  x<-samp.binary.exp( y, lambda)
  G<-as.numeric(as_factor(t(y)))-1
  out<-data.frame( x=(x-min(x))/(max(x)-min(x)),G=G)
  names(out) <- c("x","G")
  out
}

\end{verbatim}

\section{A $p=0.10$ Success for Q11}

\begin{verbatim}
> logit.q11<-data.frame( x = x, G=G)
> o = glm( G ~ x, family=binomial(ling="logit"),data=logit.q11)
Error in binomial(ling = "logit") : unused argument (ling = "logit")
> o = glm( G ~ x, family=binomial(link ="logit"),data=logit.q11)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(o)

Call:
glm(formula = G ~ x, family = binomial(link = "logit"), data = logit.q11)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2932   0.0000   0.0000   0.0000   0.2285  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   -79718      49547  -1.609    0.108
x              15943       9909   1.609    0.108

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.9976e+04  on 19076  degrees of freedom
Residual deviance: 1.6825e-01  on 19075  degrees of freedom
AIC: 4.1682

Number of Fisher Scoring iterations: 25
\end{verbatim}

\section{My Assessment of this Success}

This is a greater success and achievement than all achievements of Bill Gates between the years 1955 to 2021.  He has only produced menial trivia by comparison in this period.

\section{Some Failures and Successes}

When there is no asymmetry in frequency $p$ and $1-p$ there are difficulties.  There are also other failures of the exponential model.  I will record some successes now.

\begin{verbatim}

# We will use logistic regression
# with x variable artificially created for Q7-Q17
# We take N=500 points to determine p, lambda
# Then we use these to assign random x values
# for all the other values
# then we fit logistic regression on the (x,g)

samp.binary.exp<-function( grps, lambda ){
  grps<-as.vector(t(grps))
  n <- length(grps)
  print(n)
  gvals <- unique(grps)
  #print(gvals)
  bval <- 0
  sval <- 1
  if ( length(gvals) == 2 ){
    if ( sum(gvals==gvals[1]) >= n/2 ){
      bval <- gvals[1]
      sval <- gvals[2]
    } else {
      sval <- gvals[1]
      bval <- gvals[2]
    }
  } else {
    return(NULL)
  }
  xs<- rep( 0, n)
  for ( r in 1:n ){
    done <- F
    
    while ( done == F){
      pickx <- rexp(1,rate=1/lambda)
      #print(pickx)
      if (pickx <= 5){
        if ( grps[r] == sval){
          xs[r] <- pickx
          done <- T
        }  
      }
      if (pickx > 5){
        if ( grps[r] == bval){
          xs[r] <- pickx
          done <- T
        }  
      }
    }
  }
  xs
}

dataset.logit<-function(var,lambda){
  y<-na.omit(polv[,var])
  G<-as.numeric(as_factor(t(y)))-1
  #print(head(y))
  G<-as.numeric(as_factor(t(y)))-1
  p<-sum(G==0)/length(G)
  if ( p < 0.5) {
    G<-1-G
  }
  p<-sum(G==0)/length(G)
  print(p)
  
  x<-samp.binary.exp( G, lambda)
  out<-data.frame( x=x,G=G)
  names(out) <- c("x","G")
  out
}

dataset.logit.fixed.lambda<-function(var){
  y<-na.omit(polv[,var])
  G<-as.numeric(as_factor(t(y)))-1
  p<-sum(G==0)/length(G)
  if ( p < 0.5) {
    G<-1-G
  }
  p<-sum(G==0)/length(G)
  print(p)
  lambda <- 2*log( p/(1-p) )
  #print(head(y))
  x<-samp.binary.exp( y, lambda)
  out<-data.frame( x=x,G=G)
  names(out) <- c("x","G")
  out
}

lq14<-dataset.logit.fixed.lambda("Q14")
mq14 = glm( G ~ x, family="binomial",data=lq14)
summary(mq14)

> summary(mq13)

Call:
glm(formula = G ~ x, family = "binomial", data = lq13)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.4388   0.0000   0.0000   0.0000   0.5289  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   -83257      52211  -1.595    0.111
x             341065     213883   1.595    0.111

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.3156e+04  on 19076  degrees of freedom
Residual deviance: 4.7344e-01  on 19075  degrees of freedom
AIC: 4.4734

Number of Fisher Scoring iterations: 25

> summary(mq14)

Call:
glm(formula = G ~ x, family = "binomial", data = lq14)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2713   0.0000   0.0000   0.0000   0.2460  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)   -71933      43085   -1.67    0.095 .
x              14387       8617    1.67    0.095 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.3663e+04  on 19076  degrees of freedom
Residual deviance: 1.6948e-01  on 19075  degrees of freedom
AIC: 4.1695

Number of Fisher Scoring iterations: 25
\end{verbatim}


Let's put down some bad fits.

\begin{verbatim}
> summary(mq12)

Call:
glm(formula = G ~ x, family = "binomial", data = lq12)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.02413   0.00000   0.00000   0.00000   0.01379  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)    -8150      19404   -0.42    0.674
x              27328      65031    0.42    0.674

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.5174e+04  on 19076  degrees of freedom
Residual deviance: 1.6848e-03  on 19075  degrees of freedom
AIC: 4.0017

Number of Fisher Scoring iterations: 25

> summary(mq10)

Call:
glm(formula = G ~ x, family = "binomial", data = lq10)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.08018   0.00000   0.00000   0.00000   0.05986  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   -24855      28620  -0.868    0.385
x               4972       5725   0.868    0.385

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.4090e+04  on 19076  degrees of freedom
Residual deviance: 1.4022e-02  on 19075  degrees of freedom
AIC: 4.014

Number of Fisher Scoring iterations: 25

\end{verbatim}

So Q8,Q9,Q15 have $p$ too close to 0.5.  We're getting bad fits for Q10,Q11,Q12,Q16,Q17

We get good fits for Q13, Q14.

\section{Exploration Asymmetric Bad Fits}

It's surprising to me that Asymmetric ($p>0.6$ for example) would produce bad fits to GLM with binomial logit link.

We'll explore this next.

\section{Why Failures are Part of the Show In Science}

Only total illiterate idiots think that failures are bad in Science.  What is a failure?  It is a hypothesis about Nature that does not apply to measured data. Only numbskulls with pea-brains without an iota of respect for the delicate issue of Truth of Nature thinks that failures of Hypotheses concerning actual functioning of Nature is a problem.  These morons should not be allowed anywhere near Science for they disrupt the Purity of Devotion in the Sacred Temple of Science.  And this is Science, the Devotion to the Sacred Law of Nature.

\section{A Success for Q17}

\begin{verbatim}
> lq17<-dataset.logit("Q17",2.6)
[1] 0.6545054
[1] 19077
> mq17 = glm( G ~ x, family="binomial",data=lq17)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(mq17)

Call:
glm(formula = G ~ x, family = "binomial", data = lq17)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.6882   0.0000   0.0000   0.0000   0.5703  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)    79718      47151   1.691   0.0909 .
x             -15944       9430  -1.691   0.0909 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.4595e+04  on 19076  degrees of freedom
Residual deviance: 8.2198e-01  on 19075  degrees of freedom
AIC: 4.822

Number of Fisher Scoring iterations: 25
\end{verbatim}

I will just tweak $\lambda$ by hand to see if I can make the rest come alive.

\section{A Success For Q8}

\begin{verbatim}
> lq8<-dataset.logit("Q8",2.62)
[1] 0.5428002
[1] 19077
> mq8 = glm( G ~ x, family="binomial",data=lq8)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(mq8)

Call:
glm(formula = G ~ x, family = "binomial", data = lq8)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.3227   0.0000   0.0000   0.0000   0.3483  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   -64486      41019  -1.572    0.116
x              12897       8204   1.572    0.116

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.6306e+04  on 19076  degrees of freedom
Residual deviance: 2.2705e-01  on 19075  degrees of freedom
AIC: 4.227

Number of Fisher Scoring iterations: 25
\end{verbatim}

\section{Success For Q10}

\begin{verbatim}
> lq10<-dataset.logit("Q10",2.65)
[1] 0.6739005
[1] 19077
> mq10 = glm( G ~ x, family="binomial",data=lq10)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(mq10)

Call:
glm(formula = G ~ x, family = "binomial", data = lq10)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.5018   0.0000   0.0000   0.0000   0.7284  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)  -112769      54442  -2.071   0.0383 *
x              22554      10888   2.071   0.0383 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 24089.7442  on 19076  degrees of freedom
Residual deviance:     1.1164  on 19075  degrees of freedom
AIC: 5.1164

Number of Fisher Scoring iterations: 25
\end{verbatim}

\section{Success for Q11}

\begin{verbatim}
> lq11<-dataset.logit("Q11",2.598)
[1] 0.7826178
[1] 19077
> mq11 = glm( G ~ x, family="binomial",data=lq11)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(mq11)

Call:
glm(formula = G ~ x, family = "binomial", data = lq11)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2536   0.0000   0.0000   0.0000   0.3582  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)   -80223      47451  -1.691   0.0909 .
x              16044       9490   1.691   0.0909 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.9976e+04  on 19076  degrees of freedom
Residual deviance: 2.3367e-01  on 19075  degrees of freedom
AIC: 4.2337

Number of Fisher Scoring iterations: 25
\end{verbatim}

\section{Success for Q12}

\begin{verbatim}
> lq12<-dataset.logit("Q12",2.61)
[1] 0.6284007
[1] 19077
> mq12 = glm( G ~ x, family="binomial",data=lq12)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(mq12)

Call:
glm(formula = G ~ x, family = "binomial", data = lq12)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2614   0.0000   0.0000   0.0000   0.2624  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   -67049      43036  -1.558    0.119
x              13410       8607   1.558    0.119

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.5174e+04  on 19076  degrees of freedom
Residual deviance: 1.4979e-01  on 19075  degrees of freedom
AIC: 4.1498

Number of Fisher Scoring iterations: 25
\end{verbatim}


\section{New Hypothesis}

There exists a $\lambda$ near 2.6 such that GLM with logit link produces good fits for these data.

\section{Success With Q14}

\begin{verbatim}
> lq14<-dataset.logit("Q14",2.7)
[1] 0.6886303
[1] 19077
> mq14 = glm( G ~ x, family="binomial",data=lq14)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(mq14)

Call:
glm(formula = G ~ x, family = "binomial", data = lq14)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.5052   0.0000   0.0000   0.0000   0.6095  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   -80592      50285  -1.603    0.109
x              16118      10057   1.603    0.109

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.3663e+04  on 19076  degrees of freedom
Residual deviance: 6.3093e-01  on 19075  degrees of freedom
AIC: 4.6309

Number of Fisher Scoring iterations: 25
\end{verbatim}

\section{Success for Q9}

\begin{verbatim}
> lq9<-dataset.logit("Q9",2.55)
[1] 0.5285946
[1] 19077
> mq9 = glm( G ~ x, family="binomial",data=lq9)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(mq9)

Call:
glm(formula = G ~ x, family = "binomial", data = lq9)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8816   0.0000   0.0000   0.0000   0.7879  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)    88062      52662   1.672   0.0945 .
x             -17613      10532  -1.672   0.0945 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 26383.9100  on 19076  degrees of freedom
Residual deviance:     1.5488  on 19075  degrees of freedom
AIC: 5.5488

Number of Fisher Scoring iterations: 25
\end{verbatim}

\section{Success For Q7}

\begin{verbatim}
> lq7<-dataset.logit("Q7",2.8)
[1] 0.8114483
[1] 19077
> mq7 = glm( G ~ x, family="binomial",data=lq7)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(mq7)

Call:
glm(formula = G ~ x, family = "binomial", data = lq7)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.5486   0.0000   0.0000   0.0000   0.6403  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)   -99145      53591   -1.85   0.0643 .
x              19829      10718    1.85   0.0643 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.8471e+04  on 19076  degrees of freedom
Residual deviance: 7.6356e-01  on 19075  degrees of freedom
AIC: 4.7636

Number of Fisher Scoring iterations: 25
\end{verbatim}

\section{Success for Q13}

\begin{verbatim}
> lq13<-dataset.logit("Q13",2.85)
[1] 0.7046181
[1] 19077
> mq13 = glm( G ~ x, family="binomial",data=lq13)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(mq13)

Call:
glm(formula = G ~ x, family = "binomial", data = lq13)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.3538   0.0000   0.0000   0.0000   0.3142  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)   -80273      47140  -1.703   0.0886 .
x              16055       9428   1.703   0.0886 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.3156e+04  on 19076  degrees of freedom
Residual deviance: 2.4163e-01  on 19075  degrees of freedom
AIC: 4.2416

Number of Fisher Scoring iterations: 25
\end{verbatim}

\section{Success for Q15}

\begin{verbatim}
> summary(mq15)

Call:
glm(formula = G ~ x, family = "binomial", data = lq15)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.4840   0.0000   0.0000   0.0000   0.5595  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)   -93424      54802  -1.705   0.0882 .
x              18685      10960   1.705   0.0882 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.6273e+04  on 19076  degrees of freedom
Residual deviance: 5.8049e-01  on 19075  degrees of freedom
AIC: 4.5805

Number of Fisher Scoring iterations: 25
\end{verbatim}

\section{Success For Q16}

\begin{verbatim}
> mq16 = glm( G ~ x, family="binomial",data=lq16)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> summary(mq16)

Call:
glm(formula = G ~ x, family = "binomial", data = lq16)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.3016   0.0000   0.0000   0.0000   0.2706  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)    54967      32808   1.675   0.0939 .
x             -10994       6562  -1.675   0.0938 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.1353e+04  on 19076  degrees of freedom
Residual deviance: 1.8695e-01  on 19075  degrees of freedom
AIC: 4.1869

Number of Fisher Scoring iterations: 25
\end{verbatim}



\end{document}
